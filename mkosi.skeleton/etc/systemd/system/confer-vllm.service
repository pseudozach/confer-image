[Unit]
Description=Confer vLLM Service
After=local-fs.target mnt-config.mount mnt-models.mount nvidia-persistenced.service nvidia-fabricmanager.service nvidia-cc-attestation.service
Wants=mnt-models.mount
Requires=mnt-config.mount nvidia-fabricmanager.service nvidia-cc-attestation.service
Before=confer-proxy.service

[Service]
Type=simple
User=root
RuntimeDirectory=vllm
WorkingDirectory=/run/vllm
EnvironmentFile=/mnt/config/secrets.env
ExecStartPre=/bin/mkdir -p /tmp/huggingface /tmp/.cache
ExecStart=/bin/bash -c 'exec env -i \
    PATH=/usr/local/bin:/usr/bin:/bin \
    HOME=/tmp \
    HF_HUB_CACHE=/tmp/huggingface \
    XDG_CACHE_HOME=/tmp/.cache \
    /usr/bin/python3 -m vllm.entrypoints.openai.api_server \
    --model "$VLLM_MODEL" \
    --served-model-name "$VLLM_SERVED_MODEL_NAME" \
    --host "$VLLM_HOST" \
    --port "$VLLM_PORT" \
    --gpu-memory-utilization "$VLLM_GPU_MEMORY" \
    --tensor-parallel-size "$VLLM_TENSOR_PARALLEL_SIZE" \
    --enable-expert-parallel \
    --enable-auto-tool-choice \
    --tool-call-parser "$VLLM_TOOL_CALL_PARSER" \
    --max-model-len "$VLLM_MAX_MODEL_LEN" \
    --max-num-seqs "$VLLM_MAX_NUM_SEQS" \
    --max-num-batched-tokens "$VLLM_MAX_NUM_BATCHED_TOKENS" \
    $( [[ "${VLLM_ENABLE_CHUNKED_PREFILL:-1}" =~ ^(1|true|yes)$ ]] && echo --enable-chunked-prefill ) \
    $( [[ "${VLLM_ENABLE_PREFIX_CACHING:-0}" =~ ^(1|true|yes)$ ]] && echo --enable-prefix-caching )'
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
